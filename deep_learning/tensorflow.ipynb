{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 9 – Up and running with TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The evaluation in instant traditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = x*x*y + y + 2\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "# Check the tensorflow version\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better way\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another better way\n",
    "\n",
    "init = tf.global_variables_initializer() # prepare an init node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually initialize all the variables\n",
    "    result = f.eval()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x7fbeb8acf550>\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()\n",
    "\n",
    "print(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To reset the default group\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of a Node Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Run previous code efficiently\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.21428571],\n",
       "       [1.07142857]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1,5/3],[1, 1/9]])\n",
    "y = np.array([[12/3],[21/9]])\n",
    "theta_best = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7185181e+01],\n",
       "       [ 4.3633747e-01],\n",
       "       [ 9.3952334e-03],\n",
       "       [-1.0711310e-01],\n",
       "       [ 6.4479220e-01],\n",
       "       [-4.0338000e-06],\n",
       "       [-3.7813708e-03],\n",
       "       [-4.2348403e-01],\n",
       "       [-4.3721911e-01]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate theta using normal equation in TensorFlow\n",
    "# We will use housing dataset of end-to-end project\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compare it with pure NumPy\n",
    "# Calculate theta using normal equation in NumPy\n",
    "\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compare it with pure NumPy\n",
    "# Calculate theta using Scikit-Learn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
      " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
      " -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.11111111111111005\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "# Step 1- Normalize the feature vector list using Scikit-Learn\n",
    "# Gradient Descent requires scaling the feature vectors first\n",
    "# We could do this using TF, but let's just use Scikit-Learn for now.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.71450067\n",
      "Epoch 200 MSE = 0.5667049\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.53650916\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401656],\n",
       "       [-0.34770885],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.66145283],\n",
       "       [-0.6375278 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2- Manually computing the gradient\n",
    "# The Code is self-explanatory, except for a few new elements\n",
    "\n",
    "'''\n",
    "\n",
    "1. random_uniform() - The random_uniform() function creates a node in the graph that will generate a tensor containing\n",
    "random values, given its shape and value range, much like NumPy’s rand() function.\n",
    "\n",
    "2. assign() - The assign() function creates a node that will assign a new value to a variable. In this case, it\n",
    "implements the Batch Gradient Descent step θ(next step) = θ – η∇θMSE(θ).\n",
    "\n",
    "3. The main loop executes the training step over and over again (n_epochs times), and every 100 iterations it prints \n",
    "out the current Mean Squared Error (mse). We can see the MSE go down at every iteration\n",
    "\n",
    "'''\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbe8402ac88>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD/CAYAAADrE0HrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wEvkuaauSctWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe8c2e1fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us understand the differentiation\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate x\n",
    "x = np.linspace(-10, 10, 100)\n",
    "# Compute y\n",
    "y = x*x\n",
    "\n",
    "# plot the parabola\n",
    "plt.plot(x, y)\n",
    "\n",
    "#Lets draw a tangent at x0 = 5\n",
    "x0 = 5\n",
    "m = slope = 2*x0 # d(x^2)/dx\n",
    "\n",
    "y1 = m*x-25\n",
    "plt.plot(x, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# The above method was possible only for the cases where computing differential was easy. We can use autodiff.\n",
    "# Lets try. Now, lets compute the \n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "x = tf.constant(5.0)\n",
    "y = tf.square(x)\n",
    "z = tf.gradients(y, x)\n",
    "with tf.Session() as s:\n",
    "    print(z[0].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as above except for the `gradients = ...` line:\n",
    "# Compute gradients using autodiff\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n",
      "Best theta:\n",
      "[[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.6614528 ]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the partial derivatives of the following function with regards to `a` and `b`\n",
    "\n",
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754914"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21253741\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `GradientDescentOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n",
      "Best theta:\n",
      "[[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.6614528 ]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.068558  ]\n",
      " [ 0.8296286 ]\n",
      " [ 0.11875337]\n",
      " [-0.26554456]\n",
      " [ 0.3057109 ]\n",
      " [-0.00450251]\n",
      " [-0.03932662]\n",
      " [-0.89986444]\n",
      " [-0.87052065]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding data to the training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's implement mini-batch using placeholder nodes\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Let's make X and y as placeholder nodes\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the batch size and compute the total number of batches\n",
    "# Define function to fetch batch\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index) \n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        , -0.94235915,  1.06160074, -0.45870257,  0.04425393,\n",
       "         -0.19380963, -0.1004992 ,  1.03382082, -1.34280914]]),\n",
       " array([[2.267]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_batch(1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36894485],\n",
       "       [ 0.07636866],\n",
       "       [ 0.6792061 ],\n",
       "       [-0.11886713],\n",
       "       [ 0.84787256],\n",
       "       [ 0.6232598 ],\n",
       "       [ 0.19132051],\n",
       "       [ 0.27507773],\n",
       "       [-0.52806485]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is execution phase\n",
    "# Fetch the mini-batches one by one then provide the value of x and y via the feed_dict parameter\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n"
     ]
    }
   ],
   "source": [
    "# Let's save a model\n",
    "# Code for GradientDescentOptimizer\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       \n",
    "learning_rate = 0.01                                                                  \n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            \n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")           \n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      \n",
    "error = y_pred - y                                                                    \n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            \n",
    "training_op = optimizer.minimize(mse)                                                \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() # Create a Saver node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                            \n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\") # Save checkpoint during training\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\") # Save model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.6614528 ],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Restore model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.allclose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if restored theta is same as the saved theta\n",
    "\n",
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.Saver?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default a Saver saves and restores all variables under their own name, but if you need more control, you can specify which variables to save or restore, and what names to use. For example, the following\n",
    "Saver will save or restore only the theta variable under the name weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If we want to have a saver that loads and restores `theta` with a different name, such as `\"weights\"`\n",
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the saver also saves the graph structure itself in a second file with the extension `.meta`. You can use the function `tf.train.import_meta_graph()` to restore the graph structure. This function loads the graph into the default graph and returns a `Saver` that can then be used to restore the graph state (i.e., the variable values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# notice that we start with an empty graph.\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")  # this restores the graph's state\n",
    "    best_theta_restored = theta.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that you can import a pretrained model without having to have the corresponding Python code to build the graph. This is very handy when you keep tweaking and saving your model: you can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the graph\n",
    "## inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/random_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/random_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/random_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/random_normal/RandomStandardNormal&quot;\\n  op: &quot;RandomStandardNormal&quot;\\n  input: &quot;relu/random_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 4\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/random_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;relu/random_normal/RandomStandardNormal&quot;\\n  input: &quot;relu/random_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/random_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu/random_normal/mul&quot;\\n  input: &quot;relu/random_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu/weights&quot;\\n  input: &quot;relu/random_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/bias/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu/bias&quot;\\n  input: &quot;relu/bias/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;relu/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/z&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu/MatMul&quot;\\n  input: &quot;relu/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/max/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu/max&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;relu/z&quot;\\n  input: &quot;relu/max/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/random_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/random_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/random_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/random_normal/RandomStandardNormal&quot;\\n  op: &quot;RandomStandardNormal&quot;\\n  input: &quot;relu_1/random_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 21\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/random_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;relu_1/random_normal/RandomStandardNormal&quot;\\n  input: &quot;relu_1/random_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/random_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu_1/random_normal/mul&quot;\\n  input: &quot;relu_1/random_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu_1/weights&quot;\\n  input: &quot;relu_1/random_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_1/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu_1/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_1/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/bias/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu_1/bias&quot;\\n  input: &quot;relu_1/bias/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu_1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;relu_1/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/z&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu_1/MatMul&quot;\\n  input: &quot;relu_1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/max/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_1/max&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;relu_1/z&quot;\\n  input: &quot;relu_1/max/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/random_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/random_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/random_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/random_normal/RandomStandardNormal&quot;\\n  op: &quot;RandomStandardNormal&quot;\\n  input: &quot;relu_2/random_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 38\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/random_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;relu_2/random_normal/RandomStandardNormal&quot;\\n  input: &quot;relu_2/random_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/random_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu_2/random_normal/mul&quot;\\n  input: &quot;relu_2/random_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu_2/weights&quot;\\n  input: &quot;relu_2/random_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_2/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu_2/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_2/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/bias/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu_2/bias&quot;\\n  input: &quot;relu_2/bias/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu_2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;relu_2/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/z&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu_2/MatMul&quot;\\n  input: &quot;relu_2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/max/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_2/max&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;relu_2/z&quot;\\n  input: &quot;relu_2/max/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/random_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/random_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/random_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/random_normal/RandomStandardNormal&quot;\\n  op: &quot;RandomStandardNormal&quot;\\n  input: &quot;relu_3/random_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 55\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/random_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;relu_3/random_normal/RandomStandardNormal&quot;\\n  input: &quot;relu_3/random_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/random_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu_3/random_normal/mul&quot;\\n  input: &quot;relu_3/random_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu_3/weights&quot;\\n  input: &quot;relu_3/random_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_3/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu_3/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_3/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/bias/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu_3/bias&quot;\\n  input: &quot;relu_3/bias/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu_3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;relu_3/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/z&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu_3/MatMul&quot;\\n  input: &quot;relu_3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/max/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_3/max&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;relu_3/z&quot;\\n  input: &quot;relu_3/max/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/random_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\003\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/random_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/random_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/random_normal/RandomStandardNormal&quot;\\n  op: &quot;RandomStandardNormal&quot;\\n  input: &quot;relu_4/random_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 72\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/random_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;relu_4/random_normal/RandomStandardNormal&quot;\\n  input: &quot;relu_4/random_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/random_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu_4/random_normal/mul&quot;\\n  input: &quot;relu_4/random_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/weights&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/weights/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu_4/weights&quot;\\n  input: &quot;relu_4/random_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_4/weights&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/weights/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu_4/weights&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_4/weights&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/bias/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;relu_4/bias&quot;\\n  input: &quot;relu_4/bias/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;relu_4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@relu_4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;relu_4/weights/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/z&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;relu_4/MatMul&quot;\\n  input: &quot;relu_4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/max/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;relu_4/max&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;relu_4/z&quot;\\n  input: &quot;relu_4/max/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;output&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;relu/max&quot;\\n  input: &quot;relu_1/max&quot;\\n  input: &quot;relu_2/max&quot;\\n  input: &quot;relu_3/max&quot;\\n  input: &quot;relu_4/max&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                                        \n",
    "    sess.run(init)                                                               \n",
    "\n",
    "    for epoch in range(n_epochs):                                                 \n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36894485],\n",
       "       [ 0.07636866],\n",
       "       [ 0.6792061 ],\n",
       "       [-0.11886713],\n",
       "       [ 0.84787256],\n",
       "       [ 0.6232598 ],\n",
       "       [ 0.19132051],\n",
       "       [ 0.27507773],\n",
       "       [-0.52806485]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 0.36894485]\n",
      " [ 0.07636866]\n",
      " [ 0.6792061 ]\n",
      " [-0.11886713]\n",
      " [ 0.84787256]\n",
      " [ 0.6232598 ]\n",
      " [ 0.19132051]\n",
      " [ 0.27507773]\n",
      " [-0.52806485]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a_1\n",
      "param/a\n",
      "param_1/a\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")      # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")      # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")  # name == \"param/a\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")  # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ugly flat code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z1, 0., name=\"relu2\")  # Oops, cut&paste error! Did you spot it?\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, using a function to build the ReLUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better using name scopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                             # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      # not shown\n",
    "        return tf.maximum(z, 0., name=\"max\")                          # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing a `threshold` variable the classic way, by defining it outside of the `relu()` function then passing it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     # create the shared variable\n",
    "    scope.reuse_variables()  # then reuse it\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu8\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    x0 = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0., name=\"x\")\n",
    "    x2 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "\n",
    "print(\"x0:\", x0.op.name)\n",
    "print(\"x1:\", x1.op.name)\n",
    "print(\"x2:\", x2.op.name)\n",
    "print(\"x3:\", x3.op.name)\n",
    "print(\"x4:\", x4.op.name)\n",
    "print(\"x5:\", x5.op.name)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `variable_scope()` block first creates the shared variable `x0`, named `my_scope/x`. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables `x1` and `x2` have a name with a prefix `my_scope/`. Note however that TensorFlow makes their names unique by adding an index: `my_scope/x_1` and `my_scope/x_2`.\n",
    "\n",
    "The second `variable_scope()` block reuses the shared variables in scope `my_scope`, which is why `x0 is x3`. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (`my_scope_1`) and thus the variable `x4` is named `my_scope_1/x`.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable `my_scope/x` by creating a `variable_scope()` at the root scope (whose name is an empty string), then calling `get_variable()` with the full name of the shared variable (i.e. `\"my_scope/x\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some café?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Home-Made Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", f.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients\n",
    "### Mathematical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dx = Mul(Const(2), Mul(x, y))  # df/dx = 2xy\n",
    "df_dy = Add(Mul(x, x), Const(1))  # df/dy = x² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradients(func, vars_list, eps=0.0001):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func.evaluate()\n",
    "    for var in vars_list:\n",
    "        original_value = var.value\n",
    "        var.value = var.value + eps\n",
    "        tweaked_func_eval = func.evaluate()\n",
    "        var.value = original_value\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    return partial_derivatives\n",
    "\n",
    "df_dx, df_dy = gradients(f, [x, y])\n",
    "print(\"df/dx(3,4) =\", df_dx)\n",
    "print(\"df/dy(3,4) =\", df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Const.derive = lambda self, var: Const(0)\n",
    "Var.derive = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.derive = lambda self, var: Add(self.a.derive(var), self.b.derive(var))\n",
    "Mul.derive = lambda self, var: Add(Mul(self.a, self.b.derive(var)), Mul(self.a.derive(var), self.b))\n",
    "\n",
    "x = Var(3.0, name=\"x\")\n",
    "y = Var(4.0, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "df_dx = f.derive(x)  # 2xy\n",
    "df_dy = f.derive(y)  # x² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation (autodiff) – forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}ε\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3 + 4 \\epsilon) = 6 + 4\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3 + DualNumber(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(3 + 4ε)\\times(5 + 7ε) = 3 \\times 5 + 3 \\times 7ε + 4ε \\times 5 + 4ε \\times 7ε = 15 + 21ε + 20ε + 28ε^2 = 15 + 41ε + 28 \\times 0 = 15 + 41ε$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0, 1.0)  # 3 + ε\n",
    "y.value = DualNumber(4.0)       # 4\n",
    "\n",
    "df_dx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)       # 3\n",
    "y.value = DualNumber(4.0, 1.0)  # 4 + ε\n",
    "\n",
    "df_dy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff – Reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)\n",
    "\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", result)\n",
    "print(\"df_dx =\", x.gradient)\n",
    "print(\"df_dy =\", y.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff – reverse mode (using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3., name=\"x\")\n",
    "y = tf.Variable(4., name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "gradients = tf.gradients(f, [x, y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val, gradients_val = sess.run([f, gradients])\n",
    "\n",
    "f_val, gradients_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reciprocal()` function from SciPy's `stats` module returns a random distribution that is commonly used when you have no idea of the optimal scale of a hyperparameter. See the exercise solutions for chapter 2 for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
